{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3468a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cefc8b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#checking for device\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d88c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab9ff553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "\n",
    "#Path for training and testing directory\n",
    "train_path=r\"D:\\OUTPUT\\train\"\n",
    "test_path=r\"D:\\OUTPUT\\test\"\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b25482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories\n",
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9adfb6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Maize', 'Okra', 'Peanut']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab4874fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Network\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=3):\n",
    "        super(ConvNet,self).__init__()\n",
    "        \n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2P)/s) +1\n",
    "        \n",
    "        #Input shape= (256,3,150,150)\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,12,150,150)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "        #Shape= (256,12,150,150)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #Shape= (256,12,150,150)\n",
    "        \n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "        #Reduce the image size be factor 2\n",
    "        #Shape= (256,12,75,75)\n",
    "        \n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,20,75,75)\n",
    "        self.relu2=nn.ReLU()\n",
    "        #Shape= (256,20,75,75)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,32,75,75)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "        #Shape= (256,32,75,75)\n",
    "        self.relu3=nn.ReLU()\n",
    "        #Shape= (256,32,75,75)\n",
    "        \n",
    "        \n",
    "        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Feed forwad function\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "            \n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "            \n",
    "            \n",
    "            #Above output will be in matrix form, with shape (256,32,75,75)\n",
    "            \n",
    "        output=output.view(-1,32*75*75)\n",
    "            \n",
    "            \n",
    "        output=self.fc(output)\n",
    "            \n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afd62845",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet(num_classes=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e2e9d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03bbdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ab16629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8094e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234 78\n"
     ]
    }
   ],
   "source": [
    "print(train_count,test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96902406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: tensor(18.1535) Train Accuracy: 0.36324786324786323 Test Accuracy: 0.3333333333333333\n",
      "Epoch: 1 Train Loss: tensor(3.7730) Train Accuracy: 0.7264957264957265 Test Accuracy: 0.3333333333333333\n",
      "Epoch: 2 Train Loss: tensor(3.5965) Train Accuracy: 0.688034188034188 Test Accuracy: 0.3333333333333333\n",
      "Epoch: 3 Train Loss: tensor(2.7954) Train Accuracy: 0.7948717948717948 Test Accuracy: 0.5\n",
      "Epoch: 4 Train Loss: tensor(1.6701) Train Accuracy: 0.8461538461538461 Test Accuracy: 0.717948717948718\n",
      "Epoch: 5 Train Loss: tensor(0.8503) Train Accuracy: 0.8974358974358975 Test Accuracy: 0.5641025641025641\n",
      "Epoch: 6 Train Loss: tensor(0.3941) Train Accuracy: 0.9188034188034188 Test Accuracy: 0.6025641025641025\n",
      "Epoch: 7 Train Loss: tensor(0.4152) Train Accuracy: 0.9487179487179487 Test Accuracy: 0.5512820512820513\n",
      "Epoch: 8 Train Loss: tensor(0.2238) Train Accuracy: 0.9572649572649573 Test Accuracy: 0.5897435897435898\n",
      "Epoch: 9 Train Loss: tensor(0.0776) Train Accuracy: 0.9871794871794872 Test Accuracy: 0.6538461538461539\n"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss)+' Train Accuracy: '+str(train_accuracy)+' Test Accuracy: '+str(test_accuracy))\n",
    "    \n",
    "    #Save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),r'D:\\best_checkpoint.model')\n",
    "        best_accuracy=test_accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab879c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtest_accuracy\u001b[49m\u001b[38;5;241m.\u001b[39mtest_accuracy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_accuracy' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e818f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import squeezenet1_1\n",
    "import torch.functional as F\n",
    "from io import open\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fab424f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=r\"D:\\OUTPUT\\train\"\n",
    "pred_path=r\"D:\\CROP CLASSIFICATION1.v1i.folder\\seg_pred\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e90fef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories\n",
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5781e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Network\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=3):\n",
    "        super(ConvNet,self).__init__()\n",
    "        \n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2P)/s) +1\n",
    "        \n",
    "        #Input shape= (256,3,150,150)\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,12,150,150)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "        #Shape= (256,12,150,150)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #Shape= (256,12,150,150)\n",
    "        \n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "        #Reduce the image size be factor 2\n",
    "        #Shape= (256,12,75,75)\n",
    "        \n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,20,75,75)\n",
    "        self.relu2=nn.ReLU()\n",
    "        #Shape= (256,20,75,75)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,32,75,75)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "        #Shape= (256,32,75,75)\n",
    "        self.relu3=nn.ReLU()\n",
    "        #Shape= (256,32,75,75)\n",
    "        \n",
    "        \n",
    "        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Feed forwad function\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "            \n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "            \n",
    "            \n",
    "            #Above output will be in matrix form, with shape (256,32,75,75)\n",
    "            \n",
    "        output=output.view(-1,32*75*75)\n",
    "            \n",
    "            \n",
    "        output=self.fc(output)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "693fe1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu2): ReLU()\n",
       "  (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc): Linear(in_features=180000, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint=torch.load(r\"D:\\best_checkpoint.model\")\n",
    "model=ConvNet(num_classes=3)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e2297bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforms\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "114d7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction function\n",
    "def prediction(img_path,transformer):\n",
    "    \n",
    "    image=Image.open(img_path)\n",
    "    \n",
    "    image_tensor=transformer(image).float()\n",
    "    \n",
    "    \n",
    "    image_tensor=image_tensor.unsqueeze_(0)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor.cuda()\n",
    "        \n",
    "    input=Variable(image_tensor)\n",
    "    \n",
    "    \n",
    "    output=model(input)\n",
    "    \n",
    "    index=output.data.numpy().argmax()\n",
    "    \n",
    "    pred=classes[index]\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2a039ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path=glob.glob(pred_path+'/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4932459",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict={}\n",
    "\n",
    "for i in images_path:\n",
    "    pred_dict[i[i.rfind('/')+1:]]=prediction(i,transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f14c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c1c88d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final maize14.jpg_372135f4-4e31-40bc-ac0b-c8f1657e31b5.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final maize15.jpg_26f1aa3f-5978-4712-888e-6d88ae08fb4d.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final maize16.jpg_e65df747-ff35-4fc9-8e2e-20045f357c0a.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final maize19.jpg_3be9e29f-f04b-4702-88fb-913eed45cd41.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final maize23.jpg_00809f26-be39-4f5a-99ff-05f777fb0a23.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final maize24.jpg_dff281fa-ea5e-496d-bf91-667dc89658a1.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final maize29.jpg_26109857-01a8-462e-be7e-2b1b41232a7c.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final maize30.jpg_6320cd55-44ea-423e-a023-bf47572318e1.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final maize31.jpg_48ec7b93-4066-455d-8a9b-6317e77d1455.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final maize31.jpg_ad80d5a3-6389-449d-9ce7-5cb38f5d96e9.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final maize32.jpg_2c6692e9-2863-4ade-b605-1f3089f3737b.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final okra16.jpg_a40fe103-f945-4beb-a0c2-095530bdbe1d.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final okra17.jpg_cc5439d7-5f8a-4915-b9ed-b9d659b7e0dc.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final okra45.jpg_d5ba87d1-77bd-4739-9aa8-d33205e85aa3.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final okra9.jpg_b8c9ce81-e58f-4fda-ab16-202368860745.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final peanut14.jpg_8cac13e7-2d8c-4b27-b888-35f0ffe279ea.jpg': 'Peanut',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final peanut15.jpg_2abceebb-5297-40af-9700-26dd2db55328.jpg': 'Peanut',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final peanut16.jpg_092f3bf3-6f63-4f24-aa57-691012bd457d.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final peanut19.jpg_fb7d72bd-b392-45c7-bfe0-40f65f2aa123.jpg': 'Peanut',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final peanut30.jpg_fa939d10-87c4-4543-b1fa-045b9c7caf9f.jpg': 'Peanut',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final peanut36.jpg_d8b6423a-b005-40bc-b972-a183e3dcf5cf.jpg': 'Peanut',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final peanut65.jpg_4ce1bde8-c90d-46e1-92b4-c11a3c542289.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final peanut65.jpg_f6b84a3e-73ad-4437-9d1c-405093768aa4.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final peanut66.jpg_a018e909-54c7-4b19-9112-144ec3029324.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final peanut66.jpg_fd56d2a2-f7d9-4e6f-b51b-b004060f9811.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\agument_original_final peanut69.jpg_15dfcd14-036f-41d3-b5ff-cf56ce52179b.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize2.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize29.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize33.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize34.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize35.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize39.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize40.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize49.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize53.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize56.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize73.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize74.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize77.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize82.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final maize9.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra10.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra103.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra110.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra112.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra113.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra114.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra116.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra30.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra4.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra44.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra45.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra46.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra5.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra51.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra6.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra64.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra67.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra84.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra85.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra88.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra93.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final okra99.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut10.jpg': 'Peanut',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut2.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut24.jpg': 'Peanut',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut25.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut26.jpg': 'Peanut',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut3.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut31.jpg': 'Peanut',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut4.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut44.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut47.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut64.jpg': 'Okra',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut65.jpg': 'Peanut',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut68.jpg': 'Peanut',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut73.jpg': 'Maize',\n",
       " 'D:\\\\CROP CLASSIFICATION1.v1i.folder\\\\seg_pred\\\\final peanut9.jpg': 'Peanut'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5278e18",
   "metadata": {},
   "source": [
    "# word to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4033ae7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"D:\\P1.txt\") # can replace with df = pd.read_table('input.txt') for '\\t'\n",
    "# print(df)\n",
    "print(df.to_excel(r\"D:\\EXCEL VALUES IMAGES\\output2.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226cf38b",
   "metadata": {},
   "source": [
    "# probability predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe073d47",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'actual_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m confusion_matrix\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mcrosstab(\u001b[43mpred_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactual_class\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,pred_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_class\u001b[39m\u001b[38;5;124m\"\u001b[39m],rownames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m\"\u001b[39m],colnames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'actual_class'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "confusion_matrix=pd.crosstab(pred_dict[\"actual_class\"],pred_dict[\"predicted_class\"],rownames=[\"Actual\"],colnames=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a8aa274",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history[‘acc’]\n",
    "val_acc = history.history[‘val_acc’]\n",
    "loss = history.history[‘loss’]\n",
    "val_loss = history.history[‘val_loss’]\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, ‘r’, label=’Training acc’)\n",
    "plt.plot(epochs, val_acc, ‘b’, label=’Validation acc’)\n",
    "plt.title(‘Training and validation accuracy’)\n",
    "plt.ylabel(‘accuracy’) \n",
    "plt.xlabel(‘epoch’)\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, ‘r’, label=’Training loss’)\n",
    "plt.plot(epochs, val_loss, ‘b’, label=’Validation loss’)\n",
    "plt.title(‘Training and validation loss’)\n",
    "plt.ylabel(‘loss’) \n",
    "plt.xlabel(‘epoch’)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c45ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f284141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "    diagonal_sum = confusion_matrix.trace()\n",
    "    sum_of_all_elements = confusion_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9cab72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9bcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
